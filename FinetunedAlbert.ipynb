{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VellummyilumVinoth/Train_And_Test_Using_ALBERT/blob/main/FinetunedAlbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "id": "Vy5ShGJWzYCU",
        "outputId": "0a3e2fc9-adc2-4b1a-983b-d2b13c9c7575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "K_TYzp_HFoeH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny_LG2svTJ74",
        "outputId": "a9218915-c593-4e19-81bd-e1b2b3d62850"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "print(locale.getpreferredencoding())"
      ],
      "metadata": {
        "id": "RS9zWweAILVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af034495-0dd4-47dc-f6b5-1c7c402e2738"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UTF-8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "nR03U7UyIM1b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jJuOjdCs7OCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2955b636-d2d5-4670-b9d6-fe54701f1410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: onnxconverter-common in /usr/local/lib/python3.10/dist-packages (from transformers) (1.13.0)\n",
            "Requirement already satisfied: tf2onnx in /usr/local/lib/python3.10/dist-packages (from transformers) (1.14.0)\n",
            "Requirement already satisfied: onnxruntime>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.15.1)\n",
            "Requirement already satisfied: onnxruntime-tools>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.7.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.4.0->transformers) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.4.0->transformers) (2.0.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.4.0->transformers) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.4.0->transformers) (1.11.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools>=1.4.2->transformers) (1.14.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools>=1.4.2->transformers) (9.0.0)\n",
            "Requirement already satisfied: py3nvml in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools>=1.4.2->transformers) (0.2.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx->transformers) (1.16.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.4.0->transformers) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.10/dist-packages (from py3nvml->onnxruntime-tools>=1.4.2->transformers) (0.13.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.4.0->transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate transformers[onnx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xN53zeUfAXYF"
      },
      "outputs": [],
      "source": [
        "from transformers import AlbertForMaskedLM,RobertaTokenizerFast\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load the Roberta tokenizer and ALBERT model\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('huggingface/CodeBERTa-small-v1')\n",
        "\n",
        "model = AlbertForMaskedLM.from_pretrained('albert-base-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sG-8dRgfyhMj"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "variable_names = []\n",
        "statements = []\n",
        "\n",
        "with open('/content/drive/MyDrive/output.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        # Check if the row contains at least two columns\n",
        "        if len(row) >= 2:\n",
        "            # Append the variable name and source statement to their respective lists\n",
        "            variable_names.append(row[0].lower())\n",
        "            statements.append(row[1].replace(';', '').lower())  # Remove the ';' symbol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "87ru4eTkwo5m"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(line):\n",
        "    line = re.sub(r'-+',' ',line)\n",
        "    line = re.sub(r'[^a-zA-Z, ]+',\" \",line)\n",
        "    line = re.sub(r'[ ]+',\" \" ,line)\n",
        "    line += \"\"\n",
        "    return line\n",
        "\n",
        "source_statements = []\n",
        "len_lst = []\n",
        "for line in statements:\n",
        "    if len(line.split(\" \")) >=0:\n",
        "        line = clean_text(line)\n",
        "        source_statements.append(line)\n",
        "        len_lst.append(len(line.split(\" \")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F_P1s6m977ny"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import re\n",
        "from transformers import AdamW, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "import random\n",
        "\n",
        "class VariableNamesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, variable_names, source_statements, tokenizer, mask_probability):\n",
        "        self.variable_names = variable_names\n",
        "        self.source_statements = source_statements\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mask_probability = mask_probability\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_statements)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        variable_name = str(self.variable_names[idx])\n",
        "        source_statement = str(self.source_statements[idx])\n",
        "\n",
        "        # print(\"variable names: \",len(variable_name))\n",
        "        # print(\"source statements: \",len(source_statement))\n",
        "\n",
        "        # Tokenize the variable name and source statement\n",
        "        variable_name_tokens = self.tokenizer.tokenize(variable_name)\n",
        "        source_statement_tokens = self.tokenizer.tokenize(source_statement)\n",
        "\n",
        "        variable_name_tokens = [token.replace('Ġ', '').lower() for token in variable_name_tokens]\n",
        "        source_statement_tokens = [token.replace('Ġ', '').lower() for token in source_statement_tokens]\n",
        "\n",
        "        # print(\"Variable name tokens:\", variable_name_tokens)\n",
        "        # print(\"Source statement tokens:\", source_statement_tokens)\n",
        "\n",
        "        # Select a variable name tokens to mask\n",
        "        variable_name_indices = [i for i, token in enumerate(source_statement_tokens) if token in variable_name_tokens]\n",
        "\n",
        "        # print(\"Variable name indices:\", variable_name_indices)\n",
        "\n",
        "        num_to_mask = int(len(variable_name_indices)* self.mask_probability)\n",
        "        indices_to_mask = random.sample(variable_name_indices, num_to_mask)\n",
        "\n",
        "        # print(\"Num to mask:\", num_to_mask)\n",
        "        # print(\"Indices to mask:\", indices_to_mask)\n",
        "\n",
        "        # Replace the selected variable name tokens with the [MASK] token\n",
        "        for i in indices_to_mask:\n",
        "            source_statement_tokens[i] = '<mask>'\n",
        "\n",
        "        masked_source_statement = ' '.join(source_statement_tokens)\n",
        "        # print(\"Masked source statement:\", masked_source_statement)\n",
        "\n",
        "        # Tokenize the masked source statement\n",
        "        input_ids = self.tokenizer.encode(\n",
        "            masked_source_statement,\n",
        "            add_special_tokens=False,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "        # print(\"Input IDs:\", input_ids)\n",
        "\n",
        "        return torch.tensor(input_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-QOX_PKSAnj_"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "train_size = int(len(source_statements) * 0.9)\n",
        "train_variable_names = variable_names[:train_size]\n",
        "train_source_statements = source_statements[:train_size]\n",
        "test_variable_names = variable_names[train_size:]\n",
        "test_source_statements = source_statements[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "l5XTDrEqArjI"
      },
      "outputs": [],
      "source": [
        "# Create the train and test datasets\n",
        "train_dataset = VariableNamesDataset(train_variable_names, train_source_statements, tokenizer, mask_probability=1)\n",
        "\n",
        "test_dataset = VariableNamesDataset(test_variable_names, test_source_statements, tokenizer, mask_probability = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ozS_pGkCCvfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdaef7be-b732-4104-c0bb-921527bf98a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([437,   4,   4, 548,   4, 325, 225,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "train_dataset[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XNEOKvKz1cv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56701818-56e4-48de-f538-2a09dfa80d8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2128, 2779, 2886,  679, 3671,  772,  679, 1867, 3161, 3671,  772,  679,\n",
              "         225,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "test_dataset[25]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache"
      ],
      "metadata": {
        "id": "SmK6jw-p7EBl",
        "outputId": "db8730d7-181d-4246-900c-d9c9a5ff3bfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function torch.cuda.memory.empty_cache() -> None>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ko4wdXsy7_Vu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "outputId": "74a9470e-7ba3-4070-aa71-5f45f9ed6bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1125/1125 16:12, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.869629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.815125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.213017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.827657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.633486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.033744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.615405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.423807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.332746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.199204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.312180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.179017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.300556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.560527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.328008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.455492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.319167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.233123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.284878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.712800</td>\n",
              "      <td>7.372785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>5.712800</td>\n",
              "      <td>7.487110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>5.712800</td>\n",
              "      <td>7.476289</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1125, training_loss=5.445466281467014, metrics={'train_runtime': 974.0137, 'train_samples_per_second': 36.729, 'train_steps_per_second': 1.155, 'total_flos': 201609940377600.0, 'train_loss': 5.445466281467014, 'epoch': 15.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Prepare the data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.5\n",
        ")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    evaluation_strategy = \"steps\",   # Evaluation and Save happens every X steps\n",
        "    eval_steps = 50,                 # Evaluation happens every X steps\n",
        "    save_steps = 1000,               # Save checkpoint every X steps\n",
        "    num_train_epochs = 15,           # Total number of training epochs\n",
        "    learning_rate = 3e-5,            # Learning rate\n",
        "    per_device_train_batch_size=32,  # Batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # Batch size for evaluation\n",
        "    warmup_steps=500,\n",
        "    weight_decay = 0.01,             # Strength of weight decay\n",
        "    logging_dir='./logs',            # Directory for storing logs\n",
        "    logging_steps=1000,              # Log every X steps\n",
        ")\n",
        "\n",
        "# Instantiate the Trainer class and train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache"
      ],
      "metadata": {
        "id": "CyskenLt9hCA",
        "outputId": "5b9098c5-7bb8-42a6-dd68-e51e9eb015d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function torch.cuda.memory.empty_cache() -> None>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OOCIfwdbqe7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1af8fdc-24a1-4320-a6dd-210d7f66836e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/fine-tuned-albert5/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/fine-tuned-albert5/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/fine-tuned-albert5/vocab.json',\n",
              " '/content/drive/MyDrive/fine-tuned-albert5/merges.txt',\n",
              " '/content/drive/MyDrive/fine-tuned-albert5/added_tokens.json',\n",
              " '/content/drive/MyDrive/fine-tuned-albert5/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "output_dir = os.path.expanduser('/content/drive/MyDrive/fine-tuned-albert5')\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "# tokenizer.save_pretrained(output_dir)\n",
        "data_collator.tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2M44qMqjbmX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d118fd-a7b1-4280-916b-530cfcace263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒════════════════════╤══════════╤═════════╤════════╤══════════╤════════╤═══════════╕\n",
            "│ Fine-Tuned Model   │ <mask>   │   check │      m │   result │    res │   binding │\n",
            "╞════════════════════╪══════════╪═════════╪════════╪══════════╪════════╪═══════════╡\n",
            "│ Probability        │          │  0.6813 │ 0.0416 │   0.0332 │ 0.0125 │    0.0089 │\n",
            "╘════════════════════╧══════════╧═════════╧════════╧══════════╧════════╧═══════════╛\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AlbertForMaskedLM,RobertaTokenizerFast\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "output_dir = os.path.expanduser('/content/drive/MyDrive/fine-tuned-albert5')\n",
        "model = AlbertForMaskedLM.from_pretrained(output_dir)\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(output_dir)\n",
        "\n",
        "# Define a sample masked statement\n",
        "masked_statement = \"int <mask> = getCount();\"\n",
        "\n",
        "# Tokenize the masked statement\n",
        "input_ids = tokenizer.encode(masked_statement, add_special_tokens=False, return_tensors='pt')\n",
        "\n",
        "# Find the position of the masked token\n",
        "masked_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1][0].item()\n",
        "\n",
        "# Generate predictions for the masked token using the fine-tuned model\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "# Get the top 5 predictions and their probability scores from the fine-tuned model\n",
        "probs_ft = torch.nn.functional.softmax(predictions[0, masked_token_index], dim=-1)\n",
        "top_k_ft = torch.topk(probs_ft, k=5)\n",
        "\n",
        "# Create a table with the top predictions and their probabilities from both models\n",
        "table = [[\"Fine-Tuned Model\", f\"{tokenizer.mask_token}\"] + [tokenizer.convert_ids_to_tokens([idx])[0].replace('Ġ', '').lower() for idx in top_k_ft.indices],\n",
        "         [\"Probability\", \"\"] + [f\"{probs_ft[idx].item():.4f}\" for idx in top_k_ft.indices]]\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(table, headers=\"firstrow\", tablefmt=\"fancy_grid\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2xaquly4NRFC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JcySi56eo3UY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb82b4c-dfb7-4b17-ebc6-50bf22250219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masked statement: int <mask> = getCount();\n",
            "\n",
            "Fine-tuned model predictions:\n",
            "Prediction           Probability         \n",
            "check                0.6813\n",
            "m                    0.0416\n",
            "result               0.0332\n",
            "res                  0.0125\n",
            "binding              0.0089\n",
            "\n",
            "\n",
            "Base model predictions:\n",
            "Prediction           Probability         \n",
            "§                    0.0744\n",
            "(?:[                 0.0553\n",
            "=                    0.0517\n",
            "leg                  0.0427\n",
            "isassignablefrom     0.0412\n",
            "\n",
            "\n",
            "Masked statement: Student <mask>;\n",
            "\n",
            "Fine-tuned model predictions:\n",
            "Prediction           Probability         \n",
            "get                  0.0718\n",
            "request              0.0358\n",
            "i                    0.0301\n",
            "single               0.0286\n",
            "create               0.0275\n",
            "\n",
            "\n",
            "Base model predictions:\n",
            "Prediction           Probability         \n",
            "§                    0.0723\n",
            "(?:[                 0.0554\n",
            "=                    0.0519\n",
            "leg                  0.0445\n",
            "isassignablefrom     0.0397\n",
            "\n",
            "\n",
            "Masked statement: Person <mask> = {name: 'John'};\n",
            "\n",
            "Fine-tuned model predictions:\n",
            "Prediction           Probability         \n",
            "check                0.1651\n",
            "                     0.1444\n",
            "file                 0.0534\n",
            "int                  0.0214\n",
            "basic                0.0210\n",
            "\n",
            "\n",
            "Base model predictions:\n",
            "Prediction           Probability         \n",
            "dates                0.1008\n",
            "ask                  0.1001\n",
            "grams                0.0879\n",
            "derived              0.0351\n",
            "pro                  0.0303\n",
            "\n",
            "\n",
            "Masked statement: int[] <mask> = getNumbers();\n",
            "\n",
            "Fine-tuned model predictions:\n",
            "Prediction           Probability         \n",
            "check                0.0757\n",
            "client               0.0727\n",
            "ad                   0.0490\n",
            "type                 0.0373\n",
            "res                  0.0357\n",
            "\n",
            "\n",
            "Base model predictions:\n",
            "Prediction           Probability         \n",
            "§                    0.0765\n",
            "=                    0.0486\n",
            "(?:[                 0.0458\n",
            "hk                   0.0361\n",
            "isassignablefrom     0.0340\n",
            "\n",
            "\n",
            "Masked statement: Person[] <mask> = getPersons();\n",
            "\n",
            "Fine-tuned model predictions:\n",
            "Prediction           Probability         \n",
            "check                0.2400\n",
            "                     0.0529\n",
            "type                 0.0450\n",
            "file                 0.0411\n",
            "cx                   0.0360\n",
            "\n",
            "\n",
            "Base model predictions:\n",
            "Prediction           Probability         \n",
            "=                    0.0585\n",
            "hk                   0.0521\n",
            "§                    0.0461\n",
            "                     0.0361\n",
            "olve                 0.0297\n",
            "\n",
            "\n",
            "Masked statement: Person[] <mask> = getPersons(10, 'Sales');\n",
            "\n",
            "Fine-tuned model predictions:\n",
            "Prediction           Probability         \n",
            "check                0.4183\n",
            "type                 0.0448\n",
            "                     0.0421\n",
            "file                 0.0331\n",
            "create               0.0231\n",
            "\n",
            "\n",
            "Base model predictions:\n",
            "Prediction           Probability         \n",
            "hk                   0.0584\n",
            "borders              0.0557\n",
            "                     0.0455\n",
            "pk                   0.0326\n",
            "//                   0.0297\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AlbertForMaskedLM,RobertaTokenizerFast\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "output_dir = os.path.expanduser('/content/drive/MyDrive/fine-tuned-albert5')\n",
        "finetuned_model = AlbertForMaskedLM.from_pretrained(output_dir)\n",
        "finetuned_tokenizer = RobertaTokenizerFast.from_pretrained(output_dir)\n",
        "\n",
        "# Load the ALBERT model and tokenizer\n",
        "base_model = AlbertForMaskedLM.from_pretrained('albert-base-v2')\n",
        "base_tokenizer = RobertaTokenizerFast.from_pretrained('huggingface/CodeBERTa-small-v1')\n",
        "\n",
        "base_model.resize_token_embeddings(len(base_tokenizer))\n",
        "\n",
        "# Define a list of sample masked statements\n",
        "masked_statements = [\"int <mask> = getCount();\", \"Student <mask>;\", \"Person <mask> = {name: 'John'};\", \"int[] <mask> = getNumbers();\", \"Person[] <mask> = getPersons();\", \"Person[] <mask> = getPersons(10, 'Sales');\"]\n",
        "\n",
        "# Loop through each masked statement and generate predictions for both models\n",
        "for masked_statement in masked_statements:\n",
        "    print(f\"Masked statement: {masked_statement}\\n\")\n",
        "    # Tokenize the masked statement for the fine-tuned model\n",
        "    input_ids = finetuned_tokenizer.encode(masked_statement, add_special_tokens=False, return_tensors='pt')\n",
        "    masked_token_index = torch.where(input_ids == finetuned_tokenizer.mask_token_id)[1][0].item()\n",
        "    with torch.no_grad():\n",
        "        outputs = finetuned_model(input_ids)\n",
        "        predictions = outputs[0]\n",
        "    probs = torch.nn.functional.softmax(predictions[0, masked_token_index], dim=-1)\n",
        "    top_k = torch.topk(probs, k=5)\n",
        "    # Print the top 5 predictions and their probability scores for the fine-tuned model\n",
        "    print(\"Fine-tuned model predictions:\")\n",
        "    print(\"{:<20} {:<20}\".format('Prediction', 'Probability'))\n",
        "    for i, idx in enumerate(top_k.indices):\n",
        "        token = finetuned_tokenizer.convert_ids_to_tokens([idx])[0].replace('Ġ', '').lower()\n",
        "        prob = top_k.values[i].item()\n",
        "        print(\"{:<20} {:.4f}\".format(token, prob))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Tokenize the masked statement for the base model\n",
        "    input_ids = base_tokenizer.encode(masked_statement, add_special_tokens=False, return_tensors='pt')\n",
        "    masked_token_index = torch.where(input_ids == base_tokenizer.mask_token_id)[1][0].item()\n",
        "    with torch.no_grad():\n",
        "        outputs = base_model(input_ids)\n",
        "        predictions = outputs[0]\n",
        "    probs = torch.nn.functional.softmax(predictions[0, masked_token_index], dim=-1)\n",
        "    top_k = torch.topk(probs, k=5)\n",
        "    # Print the top 5 predictions and their probability scores for the base model\n",
        "    print(\"Base model predictions:\")\n",
        "    print(\"{:<20} {:<20}\".format('Prediction', 'Probability'))\n",
        "    for i, idx in enumerate(top_k.indices):\n",
        "        token = base_tokenizer.convert_ids_to_tokens([idx])[0].replace('Ġ', '').lower()\n",
        "        prob = top_k.values[i].item()\n",
        "        print(\"{:<20} {:.4f}\".format(token, prob))\n",
        "    print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}