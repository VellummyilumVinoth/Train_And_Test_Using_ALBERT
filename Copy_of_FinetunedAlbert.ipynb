{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VellummyilumVinoth/Train_And_Test_Using_ALBERT/blob/main/Copy_of_FinetunedAlbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "K_TYzp_HFoeH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny_LG2svTJ74",
        "outputId": "b99b4d3b-cd83-46a3-c0e0-60b95d2bb647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJuOjdCs7OCK",
        "outputId": "12489c56-9785-44bc-e089-c52d35a440b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: onnxconverter-common in /usr/local/lib/python3.10/dist-packages (from transformers) (1.13.0)\n",
            "Requirement already satisfied: tf2onnx in /usr/local/lib/python3.10/dist-packages (from transformers) (1.14.0)\n",
            "Requirement already satisfied: onnxruntime>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.15.1)\n",
            "Requirement already satisfied: onnxruntime-tools>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.7.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.4.0->transformers) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.4.0->transformers) (2.0.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.4.0->transformers) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.4.0->transformers) (1.11.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools>=1.4.2->transformers) (1.14.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools>=1.4.2->transformers) (9.0.0)\n",
            "Requirement already satisfied: py3nvml in /usr/local/lib/python3.10/dist-packages (from onnxruntime-tools>=1.4.2->transformers) (0.2.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx->transformers) (1.16.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.4.0->transformers) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.10/dist-packages (from py3nvml->onnxruntime-tools>=1.4.2->transformers) (0.13.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.4.0->transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate transformers[onnx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KHRHcHI9pqVo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Enable CUDA if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN53zeUfAXYF",
        "outputId": "b94ceda8-32b7-4abe-c5cd-183398be517b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlbertForMaskedLM(\n",
              "  (albert): AlbertModel(\n",
              "    (embeddings): AlbertEmbeddings(\n",
              "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 128)\n",
              "      (token_type_embeddings): Embedding(2, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0, inplace=False)\n",
              "    )\n",
              "    (encoder): AlbertTransformer(\n",
              "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=1024, bias=True)\n",
              "      (albert_layer_groups): ModuleList(\n",
              "        (0): AlbertLayerGroup(\n",
              "          (albert_layers): ModuleList(\n",
              "            (0): AlbertLayer(\n",
              "              (full_layer_layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (attention): AlbertAttention(\n",
              "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (attention_dropout): Dropout(p=0, inplace=False)\n",
              "                (output_dropout): Dropout(p=0, inplace=False)\n",
              "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              )\n",
              "              (ffn): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "              (ffn_output): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              (activation): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (predictions): AlbertMLMHead(\n",
              "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "    (dense): Linear(in_features=1024, out_features=128, bias=True)\n",
              "    (decoder): Linear(in_features=128, out_features=30000, bias=True)\n",
              "    (activation): NewGELUActivation()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from transformers import AlbertForMaskedLM,RobertaTokenizerFast\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Roberta tokenizer and ALBERT model\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('huggingface/CodeBERTa-small-v1')\n",
        "\n",
        "model = AlbertForMaskedLM.from_pretrained('albert-base-v2')\n",
        "\n",
        "model.to(device)  # Move the model to the specified device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sG-8dRgfyhMj"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "variable_names = []\n",
        "statements = []\n",
        "\n",
        "with open('/content/drive/MyDrive/output.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        # Check if the row contains at least two columns\n",
        "        if len(row) >= 2:\n",
        "            # Append the variable name and source statement to their respective lists\n",
        "            variable_names.append(row[0].lower())\n",
        "            statements.append(row[1].replace(';', '').lower())  # Remove the ';' symbol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "87ru4eTkwo5m"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(line):\n",
        "    line = re.sub(r'-+',' ',line)\n",
        "    line = re.sub(r'[^a-zA-Z, ]+',\" \",line)\n",
        "    line = re.sub(r'[ ]+',\" \" ,line)\n",
        "    line += \"\"\n",
        "    return line\n",
        "\n",
        "source_statements = []\n",
        "len_lst = []\n",
        "for line in statements:\n",
        "    if len(line.split(\" \")) >=0:\n",
        "        line = clean_text(line)\n",
        "        source_statements.append(line)\n",
        "        len_lst.append(len(line.split(\" \")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "F_P1s6m977ny"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import re\n",
        "from transformers import AdamW, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "import random\n",
        "\n",
        "# Create the train and test datasets\n",
        "class VariableNamesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, variable_names, source_statements, tokenizer, mask_probability):\n",
        "        self.variable_names = variable_names\n",
        "        self.source_statements = source_statements\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mask_probability = mask_probability\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_statements)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        variable_name = str(self.variable_names[idx])\n",
        "        source_statement = str(self.source_statements[idx])\n",
        "\n",
        "        # Tokenize the variable name and source statement\n",
        "        variable_name_tokens = self.tokenizer.tokenize(variable_name)\n",
        "        source_statement_tokens = self.tokenizer.tokenize(source_statement)\n",
        "\n",
        "        variable_name_tokens = [token.replace('Ġ', '').lower() for token in variable_name_tokens]\n",
        "        source_statement_tokens = [token.replace('Ġ', '').lower() for token in source_statement_tokens]\n",
        "\n",
        "        # Select a variable name tokens to mask\n",
        "        variable_name_indices = [i for i, token in enumerate(source_statement_tokens) if token in variable_name_tokens]\n",
        "\n",
        "        num_to_mask = int(len(variable_name_indices) * self.mask_probability)\n",
        "        indices_to_mask = random.sample(variable_name_indices, num_to_mask)\n",
        "\n",
        "        # Replace the selected variable name tokens with the [MASK] token\n",
        "        for i in indices_to_mask:\n",
        "            source_statement_tokens[i] = '<mask>'\n",
        "\n",
        "        masked_source_statement = ' '.join(source_statement_tokens)\n",
        "\n",
        "        # Tokenize the masked source statement\n",
        "        input_ids = self.tokenizer.encode(\n",
        "            masked_source_statement,\n",
        "            add_special_tokens=False,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=32\n",
        "        )\n",
        "\n",
        "        # Return both input_ids and labels (same as input_ids for masked language modeling)\n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids),\n",
        "            'labels': torch.tensor([input_ids[i] if i not in indices_to_mask else -100 for i in range(len(input_ids))]),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-QOX_PKSAnj_"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "train_size = int(len(source_statements) * 0.9)\n",
        "train_variable_names = variable_names[:train_size]\n",
        "train_source_statements = source_statements[:train_size]\n",
        "test_variable_names = variable_names[train_size:]\n",
        "test_source_statements = source_statements[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "l5XTDrEqArjI"
      },
      "outputs": [],
      "source": [
        "# Create the train and test datasets\n",
        "train_dataset = VariableNamesDataset(train_variable_names, train_source_statements, tokenizer, mask_probability=1)\n",
        "\n",
        "test_dataset = VariableNamesDataset(test_variable_names, test_source_statements, tokenizer, mask_probability = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozS_pGkCCvfT",
        "outputId": "256ad676-66e0-4f7e-e17e-45bde080d82d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([ 710, 1416,  741,  324,    4,  827, 5822, 7831, 3161, 5822,  973,  225,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
              " 'labels': tensor([ 710, 1416,  741, -100,    4,  827, 5822, 7831, 3161, 5822,  973,  225,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1])}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "train_dataset[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNEOKvKz1cv8",
        "outputId": "85136d11-adb9-421a-e652-549c45089a5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([ 492, 3161,  664, 3161, 1411, 3161, 2053, 3161, 1574, 3161,  664,  349,\n",
              "          338,  821,  225,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
              " 'labels': tensor([ 492, 3161,  664, 3161, 1411, 3161, 2053, 3161, 1574, 3161,  664,  349,\n",
              "          338,  821,  225,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1])}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "test_dataset[25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmK6jw-p7EBl",
        "outputId": "b018c5b4-395d-4864-d492-f0bcbe9babc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function torch.cuda.memory.empty_cache() -> None>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ko4wdXsy7_Vu",
        "outputId": "48a2f8b0-6a32-46c5-90f5-0c7e9e9bdc2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9945' max='9945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9945/9945 2:14:36, Epoch 14/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>18.460411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>16.871851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.666631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.240643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.459410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.900672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.193113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.042408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.753070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.738224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.624855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.603856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.547706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.565599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.477396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.742107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.461018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.511701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.656830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.579661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.672028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.607940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.509896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.564136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.518430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.438650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.600046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.671324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.435186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.473002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.428766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.624761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.541093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.424451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.547379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.522736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.446849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.514259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>8.051900</td>\n",
              "      <td>6.465911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.524651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.552591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.542501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.577013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.391165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.600950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.610419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.552102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.571117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.567331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.475356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.612610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.457616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.673759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.501512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.584710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.626465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.531644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.518526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>5.720200</td>\n",
              "      <td>6.539471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.543425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.571326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.540957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.500194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.491358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.469038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.593994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.556041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.559397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.505777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.561159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3550</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.536879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.517830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.532241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.552537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.599777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.572998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3850</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.431674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.545918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3950</td>\n",
              "      <td>5.608500</td>\n",
              "      <td>6.563280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.503494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4050</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.514239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.638533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4150</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.590359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.623652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.560031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.595038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4350</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.437532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.503536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4450</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.522439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.504027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4550</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.530918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.518283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4650</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.632698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.567021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4750</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.553024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.581871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4850</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.470366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.524698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4950</td>\n",
              "      <td>5.640000</td>\n",
              "      <td>6.639389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.536453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5050</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.525212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.592021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5150</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.590216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.524819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5250</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.535353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.566932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5350</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.546561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.647440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5450</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.621949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.551537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5550</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.463348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.543666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5650</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.579822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.514946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5750</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.645432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.501494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5850</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.577187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.586223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5950</td>\n",
              "      <td>5.667000</td>\n",
              "      <td>6.488593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.546484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6050</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.501497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.640525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6150</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.528161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.561369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6250</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.608503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.552904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6350</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.560792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.571791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6450</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.572709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.585391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6550</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.572707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.634338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6650</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.583058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.565526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6750</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.580840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.616356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6850</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.656880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.556789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6950</td>\n",
              "      <td>5.627800</td>\n",
              "      <td>6.666285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.547894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7050</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.552439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.700547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7150</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.572577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.600577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7250</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.532792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.590518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7350</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.588896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.619722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7450</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.737687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.605539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7550</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.583673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.778512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7650</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.636720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.558685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7750</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.554171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.598746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7850</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.577685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7900</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.655687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7950</td>\n",
              "      <td>5.676300</td>\n",
              "      <td>6.501978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.500174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8050</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.616735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.592872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8150</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.641088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8200</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.587946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8250</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.581038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8300</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.649963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8350</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.653550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.662761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8450</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.689265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.664549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8550</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.704360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8600</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.618808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8650</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.564940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.558769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8750</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.531246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.609922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8850</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.581177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8900</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.567410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8950</td>\n",
              "      <td>5.641800</td>\n",
              "      <td>6.509495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.570001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9050</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.604377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9100</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.644000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9150</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.638323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9200</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.542304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9250</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.603609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.656096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9350</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.682616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9400</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.688336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9450</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.522093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.646999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9550</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.594838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.549651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9650</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.661732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9700</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.691808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9750</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.578657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9800</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.629490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9850</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.571715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>5.681500</td>\n",
              "      <td>6.657923</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9945, training_loss=5.8932673159407996, metrics={'train_runtime': 8077.3174, 'train_samples_per_second': 9.859, 'train_steps_per_second': 1.231, 'total_flos': 197203295514624.0, 'train_loss': 5.8932673159407996, 'epoch': 14.98})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Prepare the data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.5\n",
        ")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    evaluation_strategy = \"steps\",   # Evaluation and Save happens every X steps\n",
        "    eval_steps = 50,                 # Evaluation happens every X steps\n",
        "    save_steps = 1000,               # Save checkpoint every X steps\n",
        "    num_train_epochs = 15,           # Total number of training epochs\n",
        "    learning_rate = 5e-5,            # Learning rate\n",
        "    per_device_train_batch_size=2,  # Batch size per device during training\n",
        "    per_device_eval_batch_size=4,   # Batch size for evaluation\n",
        "    warmup_steps=500,\n",
        "    weight_decay = 0.01,             # Strength of weight decay\n",
        "    logging_dir='./logs',            # Directory for storing logs\n",
        "    logging_steps=1000,              # Log every X steps\n",
        "    gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps\n",
        "    fp16=True,                      # Enable mixed-precision training\n",
        ")\n",
        "\n",
        "# Instantiate the Trainer class and train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Clear GPU memory\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()  # Clear GPU memory\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predictions = predictions.predictions.argmax(-1)  # Get the predicted token indices\n",
        "true_labels = predictions[0]  # Assuming 'predictions' is a tuple\n",
        "\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(true_labels.flatten(), test_dataset[:]['input_ids'].flatten())\n",
        "\n",
        "# Convert accuracy to percentage and round off to two decimal places\n",
        "accuracy_percentage = round(accuracy * 100, 2)\n",
        "\n",
        "print(f\"Accuracy Score: {accuracy_percentage}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "iBMFNXEs1ofQ",
        "outputId": "452d6adf-2f05-40b4-995e-a63a1536bf1b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2100 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOCIfwdbqe7s",
        "outputId": "0e9a5387-054d-48cd-b399-a4f6b097e7e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/fine-tuned-albert5/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/fine-tuned-albert5/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/fine-tuned-albert5/vocab.json',\n",
              " '/content/drive/MyDrive/fine-tuned-albert5/merges.txt',\n",
              " '/content/drive/MyDrive/fine-tuned-albert5/added_tokens.json',\n",
              " '/content/drive/MyDrive/fine-tuned-albert5/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "output_dir = os.path.expanduser('/content/drive/MyDrive/fine-tuned-albert5')\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "# tokenizer.save_pretrained(output_dir)\n",
        "data_collator.tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M44qMqjbmX-",
        "outputId": "55bf1c6a-0807-4473-8fa2-2684b526b191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒════════════════════╤══════════╤════════╤════════╤══════════╤════════╤══════════╕\n",
            "│ Fine-Tuned Model   │ <mask>   │        │      , │   string │    int │   string │\n",
            "╞════════════════════╪══════════╪════════╪════════╪══════════╪════════╪══════════╡\n",
            "│ Probability        │          │ 0.1188 │ 0.0662 │   0.0305 │ 0.0279 │    0.018 │\n",
            "╘════════════════════╧══════════╧════════╧════════╧══════════╧════════╧══════════╛\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AlbertForMaskedLM,RobertaTokenizerFast\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "output_dir = os.path.expanduser('/content/drive/MyDrive/fine-tuned-albert5')\n",
        "model = AlbertForMaskedLM.from_pretrained(output_dir)\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(output_dir)\n",
        "\n",
        "# Define a sample masked statement\n",
        "masked_statement = \"int <mask> = getCount();\"\n",
        "\n",
        "# Tokenize the masked statement\n",
        "input_ids = tokenizer.encode(masked_statement, add_special_tokens=False, return_tensors='pt')\n",
        "\n",
        "# Find the position of the masked token\n",
        "masked_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1][0].item()\n",
        "\n",
        "# Generate predictions for the masked token using the fine-tuned model\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "# Get the top 5 predictions and their probability scores from the fine-tuned model\n",
        "probs_ft = torch.nn.functional.softmax(predictions[0, masked_token_index], dim=-1)\n",
        "top_k_ft = torch.topk(probs_ft, k=5)\n",
        "\n",
        "# Create a table with the top predictions and their probabilities from both models\n",
        "table = [[\"Fine-Tuned Model\", f\"{tokenizer.mask_token}\"] + [tokenizer.convert_ids_to_tokens([idx])[0].replace('Ġ', '').lower() for idx in top_k_ft.indices],\n",
        "         [\"Probability\", \"\"] + [f\"{probs_ft[idx].item():.4f}\" for idx in top_k_ft.indices]]\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(table, headers=\"firstrow\", tablefmt=\"fancy_grid\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xaquly4NRFC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcySi56eo3UY",
        "outputId": "fcbb1ada-a477-42ab-a58d-7b44ec74bdda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masked statement: int <mask> = getCount();\n",
            "\n",
            "Fine-tuned model predictions:\n",
            "Prediction           Probability         \n",
            "                     0.1188\n",
            ",                    0.0662\n",
            "string               0.0305\n",
            "int                  0.0279\n",
            "string               0.0180\n",
            "\n",
            "\n",
            "Base model predictions:\n",
            "Prediction           Probability         \n",
            "§                    0.0753\n",
            "(?:[                 0.0576\n",
            "=                    0.0543\n",
            "leg                  0.0459\n",
            "isassignablefrom     0.0423\n",
            "\n",
            "\n",
            "Masked statement: Student <mask>;\n",
            "\n",
            "Fine-tuned model predictions:\n",
            "Prediction           Probability         \n",
            "                     0.1188\n",
            ",                    0.0662\n",
            "string               0.0305\n",
            "int                  0.0279\n",
            "string               0.0180\n",
            "\n",
            "\n",
            "Base model predictions:\n",
            "Prediction           Probability         \n",
            "§                    0.0751\n",
            "(?:[                 0.0576\n",
            "=                    0.0539\n",
            "leg                  0.0463\n",
            "isassignablefrom     0.0412\n",
            "\n",
            "\n",
            "Masked statement: Person <mask> = {name: 'John'};\n",
            "\n",
            "Fine-tuned model predictions:\n",
            "Prediction           Probability         \n",
            "                     0.1188\n",
            ",                    0.0662\n",
            "string               0.0305\n",
            "int                  0.0279\n",
            "string               0.0180\n",
            "\n",
            "\n",
            "Base model predictions:\n",
            "Prediction           Probability         \n",
            "§                    0.0821\n",
            "=                    0.0561\n",
            "ours                 0.0475\n",
            "                     0.0411\n",
            "(?:[                 0.0344\n",
            "\n",
            "\n",
            "Masked statement: int[] <mask> = getNumbers();\n",
            "\n",
            "Fine-tuned model predictions:\n",
            "Prediction           Probability         \n",
            "                     0.1188\n",
            ",                    0.0662\n",
            "string               0.0305\n",
            "int                  0.0279\n",
            "string               0.0180\n",
            "\n",
            "\n",
            "Base model predictions:\n",
            "Prediction           Probability         \n",
            "§                    0.0806\n",
            "=                    0.0512\n",
            "(?:[                 0.0482\n",
            "isassignablefrom     0.0358\n",
            "ours                 0.0350\n",
            "\n",
            "\n",
            "Masked statement: Person[] <mask> = getPersons();\n",
            "\n",
            "Fine-tuned model predictions:\n",
            "Prediction           Probability         \n",
            "                     0.1188\n",
            ",                    0.0662\n",
            "string               0.0305\n",
            "int                  0.0279\n",
            "string               0.0180\n",
            "\n",
            "\n",
            "Base model predictions:\n",
            "Prediction           Probability         \n",
            "on                   0.2239\n",
            "©                    0.0919\n",
            "u                    0.0291\n",
            "l                    0.0269\n",
            "^                    0.0225\n",
            "\n",
            "\n",
            "Masked statement: Person[] <mask> = getPersons(10, 'Sales');\n",
            "\n",
            "Fine-tuned model predictions:\n",
            "Prediction           Probability         \n",
            "                     0.1188\n",
            ",                    0.0662\n",
            "string               0.0305\n",
            "int                  0.0279\n",
            "string               0.0180\n",
            "\n",
            "\n",
            "Base model predictions:\n",
            "Prediction           Probability         \n",
            "pk                   0.0916\n",
            "ours                 0.0582\n",
            "olve                 0.0386\n",
            "                     0.0336\n",
            "client               0.0252\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AlbertForMaskedLM,RobertaTokenizerFast\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "output_dir = os.path.expanduser('/content/drive/MyDrive/fine-tuned-albert5')\n",
        "finetuned_model = AlbertForMaskedLM.from_pretrained(output_dir)\n",
        "finetuned_tokenizer = RobertaTokenizerFast.from_pretrained(output_dir)\n",
        "\n",
        "# Load the ALBERT model and tokenizer\n",
        "base_model = AlbertForMaskedLM.from_pretrained('albert-base-v2')\n",
        "base_tokenizer = RobertaTokenizerFast.from_pretrained('huggingface/CodeBERTa-small-v1')\n",
        "\n",
        "base_model.resize_token_embeddings(len(base_tokenizer))\n",
        "\n",
        "# Define a list of sample masked statements\n",
        "masked_statements = [\"int <mask> = getCount();\", \"Student <mask>;\", \"Person <mask> = {name: 'John'};\", \"int[] <mask> = getNumbers();\", \"Person[] <mask> = getPersons();\", \"Person[] <mask> = getPersons(10, 'Sales');\"]\n",
        "\n",
        "# Loop through each masked statement and generate predictions for both models\n",
        "for masked_statement in masked_statements:\n",
        "    print(f\"Masked statement: {masked_statement}\\n\")\n",
        "    # Tokenize the masked statement for the fine-tuned model\n",
        "    input_ids = finetuned_tokenizer.encode(masked_statement, add_special_tokens=False, return_tensors='pt')\n",
        "    masked_token_index = torch.where(input_ids == finetuned_tokenizer.mask_token_id)[1][0].item()\n",
        "    with torch.no_grad():\n",
        "        outputs = finetuned_model(input_ids)\n",
        "        predictions = outputs[0]\n",
        "    probs = torch.nn.functional.softmax(predictions[0, masked_token_index], dim=-1)\n",
        "    top_k = torch.topk(probs, k=5)\n",
        "    # Print the top 5 predictions and their probability scores for the fine-tuned model\n",
        "    print(\"Fine-tuned model predictions:\")\n",
        "    print(\"{:<20} {:<20}\".format('Prediction', 'Probability'))\n",
        "    for i, idx in enumerate(top_k.indices):\n",
        "        token = finetuned_tokenizer.convert_ids_to_tokens([idx])[0].replace('Ġ', '').lower()\n",
        "        prob = top_k.values[i].item()\n",
        "        print(\"{:<20} {:.4f}\".format(token, prob))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Tokenize the masked statement for the base model\n",
        "    input_ids = base_tokenizer.encode(masked_statement, add_special_tokens=False, return_tensors='pt')\n",
        "    masked_token_index = torch.where(input_ids == base_tokenizer.mask_token_id)[1][0].item()\n",
        "    with torch.no_grad():\n",
        "        outputs = base_model(input_ids)\n",
        "        predictions = outputs[0]\n",
        "    probs = torch.nn.functional.softmax(predictions[0, masked_token_index], dim=-1)\n",
        "    top_k = torch.topk(probs, k=5)\n",
        "    # Print the top 5 predictions and their probability scores for the base model\n",
        "    print(\"Base model predictions:\")\n",
        "    print(\"{:<20} {:<20}\".format('Prediction', 'Probability'))\n",
        "    for i, idx in enumerate(top_k.indices):\n",
        "        token = base_tokenizer.convert_ids_to_tokens([idx])[0].replace('Ġ', '').lower()\n",
        "        prob = top_k.values[i].item()\n",
        "        print(\"{:<20} {:.4f}\".format(token, prob))\n",
        "    print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}